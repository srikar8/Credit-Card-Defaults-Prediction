{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf100c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b640d8c13f411ca907024c61003181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3</td><td>application_1718317544167_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-53-167.ec2.internal:20888/proxy/application_1718317544167_0004/\" class=\"emr-proxy-link\" emr-resource=\"j-2TI3ROV37X5F3\n",
       "\" application-id=\"application_1718317544167_0004\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-57-252.ec2.internal:8042/node/containerlogs/container_1718317544167_0004_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f21df1cde50>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f9fea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040a7d22ab294629a876b6339ee22515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_string = \"\"\"\n",
    "SELECT\n",
    "    td.\"customer_id\",\n",
    "    td.\"P_2\", td.\"P_3\", td.\"S_23\", td.\"D_39\", td.\"B_1\", td.\"B_2\", td.\"B_5\", td.\"R_1\", td.\"S_3\",\n",
    "    td.\"D_41\", td.\"D_46\", td.\"D_52\", td.\"B_3\", td.\"D_43\", td.\"D_44\", td.\"B_4\", td.\"D_45\", td.\"R_2\",\n",
    "    td.\"D_47\", td.\"D_48\", td.\"B_7\", td.\"B_11\", td.\"B_12\", td.\"S_8\", td.\"D_55\", td.\"R_5\",\n",
    "    td.\"D_58\", td.\"B_14\", td.\"D_61\", td.\"D_62\", td.\"B_16\", td.\"B_18\", td.\"B_19\",\n",
    "    td.\"B_20\", td.\"D_74\", td.\"D_79\", td.\"S_20\", td.\"S_22\", td.\"D_102\", td.\"D_103\",\n",
    "    td.\"D_104\", td.\"D_115\", td.\"D_118\", td.\"D_128\", td.\"D_131\", td.\"D_139\", td.\"D_141\",\n",
    "    tl.\"target\"\n",
    "FROM \n",
    "    \"project555-srikar-db2\".\"train_data\" td\n",
    "JOIN \n",
    "    \"project555-srikar-db2\".\"train_labels\" tl\n",
    "ON\n",
    "    td.\"customer_id\" = tl.\"customer_id\"\n",
    "WHERE\n",
    "    td.\"P_2\" IS NOT NULL AND\n",
    "    td.\"P_3\" IS NOT NULL AND\n",
    "    td.\"S_23\" IS NOT NULL AND\n",
    "    td.\"D_39\" IS NOT NULL AND\n",
    "    td.\"B_1\" IS NOT NULL AND\n",
    "    td.\"B_2\" IS NOT NULL AND\n",
    "    td.\"B_5\" IS NOT NULL AND\n",
    "    td.\"R_1\" IS NOT NULL AND\n",
    "    td.\"S_3\" IS NOT NULL AND\n",
    "    td.\"D_41\" IS NOT NULL AND\n",
    "    td.\"D_46\" IS NOT NULL AND\n",
    "    td.\"D_52\" IS NOT NULL AND\n",
    "    td.\"B_3\" IS NOT NULL AND\n",
    "    td.\"D_43\" IS NOT NULL AND\n",
    "    td.\"D_44\" IS NOT NULL AND\n",
    "    td.\"B_4\" IS NOT NULL AND\n",
    "    td.\"D_45\" IS NOT NULL AND\n",
    "    td.\"R_2\" IS NOT NULL AND\n",
    "    td.\"D_47\" IS NOT NULL AND\n",
    "    td.\"D_48\" IS NOT NULL AND\n",
    "    td.\"B_7\" IS NOT NULL AND\n",
    "    td.\"B_11\" IS NOT NULL AND\n",
    "    td.\"B_12\" IS NOT NULL AND\n",
    "    td.\"S_8\" IS NOT NULL AND\n",
    "    td.\"D_55\" IS NOT NULL AND\n",
    "    td.\"R_5\" IS NOT NULL AND\n",
    "    td.\"D_58\" IS NOT NULL AND\n",
    "    td.\"B_14\" IS NOT NULL AND\n",
    "    td.\"D_61\" IS NOT NULL AND\n",
    "    td.\"D_62\" IS NOT NULL AND\n",
    "    td.\"B_16\" IS NOT NULL AND\n",
    "    td.\"B_18\" IS NOT NULL AND\n",
    "    td.\"B_19\" IS NOT NULL AND\n",
    "    td.\"B_20\" IS NOT NULL AND\n",
    "    td.\"D_74\" IS NOT NULL AND\n",
    "    td.\"D_79\" IS NOT NULL AND\n",
    "    td.\"S_20\" IS NOT NULL AND\n",
    "    td.\"S_22\" IS NOT NULL AND\n",
    "    td.\"D_102\" IS NOT NULL AND\n",
    "    td.\"D_103\" IS NOT NULL AND\n",
    "    td.\"D_104\" IS NOT NULL AND\n",
    "    td.\"D_115\" IS NOT NULL AND\n",
    "    td.\"D_118\" IS NOT NULL AND\n",
    "    td.\"D_128\" IS NOT NULL AND\n",
    "    td.\"D_131\" IS NOT NULL AND\n",
    "    td.\"D_139\" IS NOT NULL AND\n",
    "    td.\"D_141\" IS NOT NULL;\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5d18b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11f2708fc404015adf0e89aec4f3e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aws_access_key_id=\"ASIAV424PQGKAVODD4KD\"\n",
    "aws_secret_access_key=\"443RKphEPZ2eWrqy0cckly322lpajGUUdC0f+On6\"\n",
    "aws_session_token=\"IQoJb3JpZ2luX2VjEP7//////////wEaCXVzLXdlc3QtMiJIMEYCIQDCg39s5GGmnSoRVaK3aXIU7K2mDHwieqskHxhaecUpvwIhALKJbvsxhg8KGrAWZ8tl4PU+Y9cY4UcAWgpTZap4F6r8KrYCCJf//////////wEQARoMNDA1NTMxNDkyNzU2IgwN+pIMpuNW3K1ZbZYqigIxd9UmPtdQS5CN08k9j0D1Jr7Ve3j2wnDwVtYfNEK27pQ3gaZKqUmU681aIHfgAgenQkyPpJQOO+kILDOvBtIYAd+b1SzedH1XDJ1d5/RAdbCeUedji4qnQWZEKmlapxqS8lo3oqdvVgb6NQTsX7WBknZH5qhrsu4uEqVSa0v9lXxU311ng4NST7X4c1u3bjAXJraCtefMRofCIL7s0YGGthg578LESzTpCDfyAvQojsXHGcr/qfiiwCm+I91lV6ZpoYBWtKBW+FNJpF6xL1iR/Cuc5fVE6156aD0nKojsrfLVN8rbo+KB2a5gO6wwuw+r2R1kEtsrECAN+rm0yVLsG+dIyyyBJaCduDDs3q2zBjqcAZlKoVDdqmREIBSarXnbqCjhbztD62vGVRGor4TA+epcLe8iEg/edcezKhNAeQemeqBPNdKTsZ9lNGlMIFfBQj0NI7ebOAu8uv7SAW1AkEgfNwFk1K6XYFTGH7N08nqHuY6pJa/ycUnfDA2SVajvS2CvvcDzwGpwxdFhwa/r206ooFwx1uTEgbRXtTiFePKsGbkD/m3fiw03SC8bOw==\"\n",
    "AWS_REGION='us-east-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b30fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a1664555ca4d0a91e4896cb36bb7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.34.126-py3-none-any.whl (139 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "Collecting botocore<1.35.0,>=1.34.126\n",
      "  Downloading botocore-1.34.126-py3-none-any.whl (12.3 MB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/lib/python3.9/site-packages (from botocore<1.35.0,>=1.34.126->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/lib/python3.9/site-packages (from botocore<1.35.0,>=1.34.126->boto3) (1.25.10)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.126->boto3) (1.13.0)\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.34.126 botocore-1.34.126 s3transfer-0.10.1\n",
      "\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Collecting numpy>=1.22.4\n",
      "  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.13.0)\n",
      "Installing collected packages: tzdata, python-dateutil, numpy, pandas\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Not uninstalling python-dateutil at /usr/lib/python3.9/site-packages, outside environment /mnt/yarn/usercache/livy/appcache/application_1718317544167_0004/container_1718317544167_0004_01_000001/tmp/spark-4eae2ae1-5855-4747-8a3a-94adb515799e\n",
      "    Can't uninstall 'python-dateutil'. No files were found to uninstall.\n",
      "Successfully installed numpy-1.26.4 pandas-2.2.2 python-dateutil-2.9.0.post0 tzdata-2024.1\n",
      "\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./tmp/spark-4eae2ae1-5855-4747-8a3a-94adb515799e/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-10.3.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.53.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Requirement already satisfied: numpy>=1.23 in ./tmp/spark-4eae2ae1-5855-4747-8a3a-94adb515799e/lib64/python3.9/site-packages (from matplotlib) (1.26.4)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting zipp>=3.1.0\n",
      "  Downloading zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.13.0)\n",
      "Installing collected packages: zipp, pyparsing, pillow, packaging, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.0 importlib-resources-6.4.0 kiwisolver-1.4.5 matplotlib-3.9.0 packaging-24.1 pillow-10.3.0 pyparsing-3.1.2 zipp-3.19.2\n",
      "\n",
      "Requirement already satisfied: numpy in ./tmp/spark-4eae2ae1-5855-4747-8a3a-94adb515799e/lib64/python3.9/site-packages (1.26.4)\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 2.15.30 requires python-dateutil<=2.8.2,>=2.1, but you have python-dateutil 2.9.0.post0 which is incompatible.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"boto3\")\n",
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"matplotlib\")\n",
    "sc.install_pypi_package(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e67d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dd7c772d60431bb41347c878e9beda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query succeeded with execution ID: 29db6a36-0b79-437c-8898-7dda9299f9f0\n",
      "Test Accuracy: 0.8266\n",
      "Area Under ROC: 0.9060"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "def start_athena_query(query_string, s3_output):\n",
    "    \"\"\"\n",
    "    Start an Athena query and return the query execution ID.\n",
    "    \n",
    "    :param query_string: SQL query to execute\n",
    "    :param s3_output: S3 bucket URI for query output\n",
    "    :return: Query execution ID\n",
    "    \"\"\"\n",
    "    athena_client = boto3.client('athena',\n",
    "                                 aws_access_key_id=aws_access_key_id,\n",
    "                                 aws_secret_access_key=aws_secret_access_key,\n",
    "                                 aws_session_token=aws_session_token,\n",
    "                                 region_name=AWS_REGION)\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query_string,\n",
    "        ResultConfiguration={'OutputLocation': s3_output}\n",
    "    )\n",
    "    return response['QueryExecutionId']\n",
    "\n",
    "def wait_for_query_to_complete(athena_client, query_execution_id):\n",
    "    \"\"\"\n",
    "    Wait for an Athena query to complete and return the status.\n",
    "    \n",
    "    :param athena_client: Boto3 Athena client\n",
    "    :param query_execution_id: ID of the query to wait for\n",
    "    :return: Query status\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        query_status = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        status = query_status['QueryExecution']['Status']['State']\n",
    "        if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "            break\n",
    "        time.sleep(5)\n",
    "    return status, query_status\n",
    "\n",
    "def load_data_from_s3(spark, s3_output_location):\n",
    "    \"\"\"\n",
    "    Load CSV data from S3 into a Spark DataFrame.\n",
    "    \n",
    "    :param spark: Spark session\n",
    "    :param s3_output_location: S3 URI of the CSV file\n",
    "    :return: Spark DataFrame\n",
    "    \"\"\"\n",
    "    return spark.read.csv(s3_output_location, header=True)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data by dropping 'customer_id' and converting columns to double.\n",
    "    \n",
    "    :param df: Input Spark DataFrame\n",
    "    :return: Preprocessed Spark DataFrame\n",
    "    \"\"\"\n",
    "    df = df.drop('customer_id')\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "    return df\n",
    "\n",
    "def train_and_evaluate_model(df):\n",
    "    \"\"\"\n",
    "    Train and evaluate a GBTClassifier model on the input data.\n",
    "    \n",
    "    :param df: Input Spark DataFrame\n",
    "    :return: Tuple of accuracy and AUC\n",
    "    \"\"\"\n",
    "    train_data, test_data = df.randomSplit([0.8, 0.2], seed=24)\n",
    "    feature_columns = train_data.columns[:-1]  # Exclude the target column\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    gbt = GBTClassifier(labelCol=\"target\", featuresCol=\"features\", maxIter=10)\n",
    "    pipeline = Pipeline(stages=[assembler, gbt])\n",
    "    \n",
    "    model = pipeline.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    \n",
    "    evaluator_auc = BinaryClassificationEvaluator(labelCol=\"target\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    auc = evaluator_auc.evaluate(predictions)\n",
    "    \n",
    "    return accuracy, auc\n",
    "\n",
    "def main():\n",
    "    \n",
    "    s3_output = 's3://project555-srikar2/athena-spark-output/'\n",
    "    \n",
    "    query_execution_id = start_athena_query(query_string, s3_output)\n",
    "    \n",
    "    athena_client = boto3.client('athena',\n",
    "                                 aws_access_key_id=aws_access_key_id,\n",
    "                                 aws_secret_access_key=aws_secret_access_key,\n",
    "                                 aws_session_token=aws_session_token,\n",
    "                                 region_name=AWS_REGION)\n",
    "    \n",
    "    status, query_status = wait_for_query_to_complete(athena_client, query_execution_id)\n",
    "    \n",
    "    if status == 'SUCCEEDED':\n",
    "        print(f\"Query succeeded with execution ID: {query_execution_id}\")\n",
    "        result_output_location = query_status['QueryExecution']['ResultConfiguration']['OutputLocation']\n",
    "        \n",
    "        #Load\n",
    "        df = load_data_from_s3(spark, result_output_location)\n",
    "        #Preprocess\n",
    "        df = preprocess_data(df)\n",
    "        #Train & Eval\n",
    "        accuracy, auc = train_and_evaluate_model(df)\n",
    "        \n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Area Under ROC: {auc:.4f}\")\n",
    "        \n",
    "    elif status == 'FAILED':\n",
    "        print(f\"Query failed with execution ID: {query_execution_id}\")\n",
    "    elif status == 'CANCELLED':\n",
    "        print(f\"Query was cancelled with execution ID: {query_execution_id}\")\n",
    "    else:\n",
    "        print(f\"Unexpected status: {status}\")\n",
    "    \n",
    "    #spark.stop()\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8547b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67c4a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e86a76c4644a24bdb0925a93ba876b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data is uploaded\n",
      "train_labels is uploaded"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def get_destination_path(bucket_name,des_data_folder,table_name):\n",
    "    return f'{bucket_name}{des_data_folder}{table_name}/'\n",
    "\n",
    "def source_to_write(S3_SOURCE_DATA_PATH,bucket_name,des_data_folder,table_name):\n",
    "    train_df = spark.read.option(\"Header\", True).option(\"InferSchema\", True).csv(S3_SOURCE_DATA_PATH)\n",
    "\n",
    "    replacements = {c:c.replace(' ','_') for c in train_df.columns if ' ' in c}\n",
    "    data_df = train_df.select([F.col(c).alias(replacements.get(c, c)) for c in train_df.columns])\n",
    "\n",
    "    #df_casted = data_df.select([col(c).cast(DoubleType()).alias(c) for c in data_df.columns])\n",
    "    get_dst =get_destination_path(bucket_name,des_data_folder,table_name)\n",
    "    data_df.write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet(get_dst)\n",
    "    print(f'{table_name} is uploaded')\n",
    "    \n",
    "\n",
    "\n",
    "bucket_name ='s3://project555-srikar2/'\n",
    "source_data_folder = 'raw-data/'\n",
    "des_data_folder = 'transformed-data/'\n",
    "\n",
    "file_name_1 = \"train_data.csv\"\n",
    "file_name_2 = \"train_labels.csv\"\n",
    "\n",
    "S3_SOURCE_DATA_PATH_1 = f'{bucket_name}{source_data_folder}{file_name_1}'\n",
    "S3_SOURCE_DATA_PATH_2 = f'{bucket_name}{source_data_folder}{file_name_2}'\n",
    "\n",
    "source_to_write(S3_SOURCE_DATA_PATH_1,bucket_name,des_data_folder,file_name_1[:-4])\n",
    "source_to_write(S3_SOURCE_DATA_PATH_2,bucket_name,des_data_folder,file_name_2[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad5902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85279f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
